apiVersion: apps/v1
kind: Deployment
metadata:
  name: nii-inference-server-deployment
  labels:
    app: nii-inference-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nii-inference-server
  template:
    metadata:
      labels:
        app: nii-inference-server
    spec:
      containers:
      - name: nii-inference-server
        image: vtn13042000/nii_inference_service_intel:1.0.0
        ports:
        - containerPort: 9000
        resources:
          requests:
            cpu: "100m"  # Request 0.1 CPU core
            memory: "128Mi"  # Request 128 MiB RAM
          limits:
            cpu: "500m"  # Limit to 0.5 CPU core
            memory: "512Mi"  # Limit to 512 MiB RAM
        env:
        - name: minio_client_access_key
          value: "prJxdafe7L400AAXScc8"
        - name: minio_client_secret_key
          value: "2nC9WwmhZe8Ura1XaEo5My5YvXhKClgbq3SNNwV7"
        - name: architecture_file
          value: "/inference-server/data/architecture_file.json"
        - name: weights_file
          value: "/inference-server/data/weights_file.h5"
        volumeMounts:
        - name: architecture-volume
          mountPath: /inference-server/data/architecture_file.json
          subPath: architecture_file.json
        - name: weights-volume
          mountPath: /inference-server/data/weights_file.h5
          subPath: weights_file.h5
      volumes:
      - name: architecture-volume
        hostPath:
          path: /home/vtn/aalto-internship/test_model/VGG16/3/6/repair/model.json
      - name: weights-volume
        hostPath:
          path: /home/vtn/aalto-internship/test_model/VGG16/3/6/repair/model.h5

---
apiVersion: v1
kind: Service
metadata:
  name: nii-inference-server-service
spec:
  selector:
    app: nii-inference-server
  ports:
  - name: nii-inference-port
    protocol: TCP
    port: 9000
    targetPort: 9000
  type: NodePort
